# import necessary libraries
import os
import openai
import sounddevice as sd
from scipy.io.wavfile import write
import wavio as wv
import replicate

openai.api_key = "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"
os.environ['REPLICATE_API_TOKEN'] = '7b63ad7d84b75864d74799794581e3d441e3b81d'

# OpenAI program which returns chatbot replies given question or text input in Turkish.
def chat_gpt(cevir):
    completion = openai.ChatCompletion.create(
    model = 'gpt-3.5-turbo',
        messages = [
     {'role': 'user', 'content': 'Translate the following English text to Turkish:' + cevir}
        ],
    temperature = 0  
    )

    sonuc = str(completion['choices'][0]['message']['content']).lstrip()
    i = sonuc.find(":") + 2
    print(sonuc[i:])


# select replication model
model = replicate.models.get("salesforce/blip")
version = model.versions.get("2e1dddc8621f72155f24cf2e0adbde548458d3cab9f00c0139eea840d0ac4746")

# set inputs for image captioning task
inputs = {
    # Input image file path
    'image': open("\\demo.jpg", "rb"),

    # Choose a task for image captioning
    'task': "image_captioning",

    # Type question for the input image for visual question answering task if needed
    # 'question': ...,
}

# get output from the version of selected model
output = version.predict(**inputs)
chat_gpt(str(output))

p = 1
while (p == 1): 
    # prompt user for whether they want to ask questions related to the photo
    query = input('Fotoğrafla ilgili soru sormak ister misiniz? Yes/No ')
    Fl = query[0:1].lower() 
    if query == '' or not Fl in ['y','n']: 
        print('Lütfen "Yes" veya "No" olarak cevaplayın') 
     
    if Fl == 'y': 
        print("5 saniye içinde sorunuzu sorabilirsiniz")
    
        # Define audio recording properties
        freq = 44100 # Sampling frequency
        duration = 5 # Recording duration in seconds
 
        # Record audio with duration and sampling rate using sounddevice library
        recording = sd.rec(int(duration * freq),
                       samplerate=freq, channels=2)
        sd.wait() # Wait till recording completes
        print("Soru kaydedildi")
        
        # Convert the NumPy array obtained from audio recording into an audio file (.wav) using scipy library
        write("recording0.wav", freq, recording)
        
        #Convert the NumPy array obtained into an audio file (.wav) using wavio library
        wv.write("recording1.wav", recording, freq, sampwidth=2)

        # Translate audio file to text
        audio_file = open("recording1.wav", "rb")
        transcript = openai.Audio.translate("whisper-1", audio_file)
        print(str(transcript.text))
        chat_gpt(str(transcript.text))
        
        # Pass translated text as a question input based on image to obtain answer
        inputs = {
            # Input image file path
            'image': open("C:\\Users\\mert9\\Desktop\\piton\\demo.jpg", "rb"),
            
            # Choose a task for VQA
            'task': "visual_question_answering",

            # Pass the translated text generated by audio recordings for visual question answering task
            'question': str(transcript.text),
                  }

        output = version.predict(**inputs)
        chat_gpt(str(output))

    if Fl == 'n': 
        raise SystemExit
        
